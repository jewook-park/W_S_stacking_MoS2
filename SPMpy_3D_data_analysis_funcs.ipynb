{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f437719c-4926-425f-9516-42a94a705f0e",
   "metadata": {},
   "source": [
    "# SPMpy data analysis function \n",
    "## after loadig the data file & xarray converted\n",
    "\n",
    "* Authors : Dr. Jewook Park(IBS-CALDES)\n",
    "    * *IBS-CALDES (Inistitute for Basic Science,Center for Artificial Low Dimensional Electronic Systems), South Korea*\n",
    "    * email :  jewookpark@ibs.re.kr\n",
    "* Contributors : Hyoungkug Kim, Sungmin Song, Taemin Ahn (Ph.D. Candidates in Prof. Taehwan Kim group, POSTECH, Korea)     \n",
    "    * *Professor Taehwan Kim' group POSTECH (Pohang University of Science and Technology)*\n",
    "\n",
    ">> SPMpy is an python package for code-based scanning probe microscopy data analysis. Scanning probe microscopy(SPM) data set are inherently multidimensional; for example, scanning tunneling microscopy and spectroscopy (STM/S) and atomic force microscopy (AFM). To analyze scientific information in SPM data, SPMPY exploits recent image processing techniques, a.k.a. computer vision technology. Automated data processing with computational power will extract scientific knowledge from experimental data and accelerate cutting-edge research in nanoscience.\n",
    "\n",
    "* This inpynb file contains data analysis functions for **xarray-converted** data set (or PANDAS Dataframes). \n",
    "    * v.202 0125  \n",
    "    \n",
    "* The multidimensional SPM data will be coverted to Xarray DataArray (similar to PANDAS DataFrames for 3D data, pelase check ttps://xarray.pydata.org/en/stable/ for details).\n",
    "* To take advantage of SPMpy package, other types of SPM data are supposed to be converted as PANDAS DataFrame (or Xarray DataArray) in advance. \n",
    "\n",
    "\n",
    "> This notebook is intended as an example of **SPMPY** project usage.\n",
    " **SPMPY** is an open source project. \n",
    "Github : https://github.com/Jewook-Park/SPMPY\n",
    "> Data flow chart will be added sooner or later.\n",
    "( drawio  or svg file) \n",
    "\n",
    "* Contributions, comments, ideas, error reports are always welcome.  Please use the Github page or email. Comments & remarks sholud be Korean or English. \n",
    "\n",
    "\n",
    "# SPMpy data analysis function \n",
    "\n",
    "> ## Jewook's Functions\n",
    "        * SPMpy grid data analysis function for 2D data (v.0.2022 0125)\n",
    "\n",
    ">> ###  Check the file path \n",
    "    * FilesInFolder(path) $\\to$ in **SPMpy_file_loading_funcs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db7421-017c-48d1-bd78-efd28a99e26f",
   "metadata": {},
   "source": [
    "# 0.  Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1aa3d-756e-45e3-a9dd-812c00eb04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from warnings import warn\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from SPMpy_file_loading_funcs import *\n",
    "\n",
    "# some packages may be yet to be installed\n",
    "try:\n",
    "     from pptx import Presentation\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named Presentation')\n",
    "    !pip install python-pptx  \n",
    "    from pptx import Presentation\n",
    "    from pptx.util import Inches, Pt\n",
    "\n",
    "try:\n",
    "    import nanonispy as nap\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named nanonispy')\n",
    "    !pip install nanonispy\n",
    "    import nanonispy as nap\n",
    "\n",
    "try:\n",
    "    import seaborn_image as isns\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named seaborn-image')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install --upgrade seaborn-image    \n",
    "    import seaborn_image as isns\n",
    "\n",
    "try:\n",
    "    import xarray as xr\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named xarray')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install xarray \n",
    "    import xarray as xr\n",
    "    \n",
    "try:\n",
    "    import xrft\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named xrft')\n",
    "    !pip install xrft \n",
    "    import xrft\n",
    "\n",
    "try:\n",
    "    import seaborn_image as isns\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named seaborn-image')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install --upgrade seaborn-image    \n",
    "    import seaborn_image as isns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdc899-62f3-4d28-ae59-d713498377d1",
   "metadata": {},
   "source": [
    "# Grid data analysis functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc6e12-10e2-48bb-b659-12566b79ecf8",
   "metadata": {},
   "source": [
    "# find the gap region \n",
    "* gap size \n",
    "    * based on measurement error (I limit ~ 1E-11pA) or (Lock-in resolution limnit ~ 1E-11 pA ) find the gapped region in spectroscopy \n",
    "* calibrated LDOS \n",
    "    * convert the LIX values (from Lockin) as dI/dV unit [A/V] \n",
    "    * check the LIX offest value based on calibrated dI/dV at I=0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf452793-17d4-4085-ac1b-bf21a353b3d1",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_3D_Gap(grid_3D, I_0_pA = 1E-13 ,LIX_0_pA = 1E-14):\n",
    "    grid_3D_prcssd = grid_3D.copy(deep = True)\n",
    "    # I_fb values less than I_min_A => zero value \n",
    "    I_0_pA = I_0_pA\n",
    "\n",
    "    gap_mask_I  = np.abs(grid_3D.I_fb) < I_0_pA\n",
    "    gap_I =  grid_3D.I_fb.where(gap_mask_I)\n",
    "\n",
    "    CBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).max('bias_mV') # CBM_I_mV\n",
    "    VBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).min('bias_mV') # VBM_I_mV\n",
    "    # map of the CBM&VBM energy (bias_mV)   \n",
    "    gap_size_I =  (CBM_I_mV - VBM_I_mV ) \n",
    "    # from VBM to CBM energy gap size in mV ( index differnce * bias_mV step size) \n",
    "\n",
    "    grid_3D_prcssd['CBM_I_mV'] = CBM_I_mV\n",
    "    grid_3D_prcssd['VBM_I_mV'] = VBM_I_mV\n",
    "    grid_3D_prcssd['gap_size_I'] = gap_size_I\n",
    "    ###############################################\n",
    "\n",
    "    grid_3D['dIdV'] = grid_3D.differentiate(coord = 'bias_mV').I_fb\n",
    "    # numerically calculated dI/dV from I_fb\n",
    "    LIX_ratio = grid_3D.dIdV / grid_3D.LIX_fb\n",
    "       \n",
    "    grid_3D['LIX_unit_calc'] = np.abs( LIX_ratio.mean())*grid_3D.LIX_fb\n",
    "    # LIX unit calibration \n",
    "    # pA unit : lock-in result \n",
    "    # LIX_unit_calc : calibrated as [A/V] unit for dI/dV\n",
    "    \n",
    "    \n",
    "    # LIX_fb values less than LIX_min_A => zero value \n",
    "    # LIX_0_pA = LIX_0_pA\n",
    "    LIX_0_AV  =  LIX_0_pA * LIX_ratio.mean()\n",
    "    # calibrated LIX resolution limit\n",
    "    gap_mask_LIX  = np.abs(grid_3D.LIX_unit_calc) < LIX_0_AV\n",
    "    # gap_mask_LIX  = np.abs(grid_3D.LIX_fb) > LIX_0_pA\n",
    "    # because of the same coefficient ('LIX_ratio.mean()')\n",
    "    # range for CBM &VBM is not different between  LIX_unit_calc & LIX_fb\n",
    "    # 3D mask \n",
    "\n",
    "    LIX_unit_calc_offst = grid_3D.dIdV.where(gap_mask_I).mean()- grid_3D['LIX_unit_calc'].where(gap_mask_I).mean()\n",
    "    # possible LIX offset adjust (based on dI/dV calc value)\n",
    "    grid_3D_prcssd['LDOS_fb'] = grid_3D.LIX_unit_calc + LIX_unit_calc_offst\n",
    "    # assign dI/dV value at I=0  as a reference offset \n",
    "    # grid_3D['LDOS_fb'] is calibrated dIdV with correct unit ([A/V]) for LDOS \n",
    "    # LDOS_fb is proportional to the real LDOS\n",
    "    # here we dont consider the matrix element for\n",
    "    grid_3D_prcssd['CBM_LIX_mV'] = grid_3D.bias_mV.where(gap_mask_LIX).max('bias_mV').fillna(0)\n",
    "    \n",
    "    #CBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).max('bias_mV') # CBM_I_mV\n",
    "\n",
    "    \n",
    "    grid_3D_prcssd['VBM_LIX_mV'] = grid_3D.bias_mV.where(gap_mask_LIX).min('bias_mV').fillna(0)\n",
    "    # CBM& VBM bias_mV value ,  fill nan as (0)\n",
    "    gap_size_LIX = grid_3D_prcssd.CBM_LIX_mV - grid_3D_prcssd.VBM_LIX_mV\n",
    "    grid_3D_prcssd['gap_size_LIX'] = gap_size_LIX\n",
    "    # cf) use the 'isin' to find zero & find the index with 'np.argwhere'\n",
    "\n",
    "    # (c.f) to fill the VBM, \n",
    "    #grid_3D_prcssd['VBM_LIX'] = grid_3D.LIX_fb.where(gap_mask_LIX).fillna(10000).argmin(dim = 'bias_mV')\n",
    "    # apply find gap_mask_LIX(gapped area), fill 'nan' as -10000 & find argmax \n",
    "    #grid_3D_prcssd['VBM_LIX'] = grid_3D.VBM_LIX.where(~gap_mask_LIX).fillna(np.argwhere(grid_3D_prcssd.bias_mV.isin(0).values)[0,0])\n",
    "    # for the metallic area (~gap_mask_LIX), fill the bias_mV index of 'zero_bias'\n",
    "    #grid_3D_prcssd['VBM_LIX']=  grid_3D_prcssd['VBM_LIX'].astype(np.int64)\n",
    "    # the same dtype as np.int64\n",
    "    grid_3D_prcssd.attrs['I[A]_limit'] = I_0_pA\n",
    "    grid_3D_prcssd.attrs['LDOS[A/V]_limit'] = LIX_0_AV.values\n",
    "    \n",
    "    ## split  LIX_fb_offst as a CB & VB region \n",
    "    \n",
    "    grid_3D_prcssd['LDOS_fb_CB'] =  grid_3D_prcssd.LDOS_fb.where(\n",
    "        grid_3D_prcssd.bias_mV  >  grid_3D_prcssd.CBM_LIX_mV\n",
    "    )\n",
    "    grid_3D_prcssd['LDOS_fb_VB'] =  grid_3D_prcssd.LDOS_fb.where(\n",
    "        grid_3D_prcssd.bias_mV  <  grid_3D_prcssd.VBM_LIX_mV\n",
    "    )\n",
    "    \n",
    "    return grid_3D_prcssd\n",
    "\n",
    "#test\n",
    "#grid_3D_gap = grid_3D_Gap(grid_3D)\n",
    "#grid_3D_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409266f-d117-4260-b6a5-843a9efecdb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Signal Treatments \n",
    "* Assume that the coords in Xarray are 'X\",'Y','bias_mV'\n",
    "* if the xr is 2D array ==> (X,bias_mV) or (Y, bias_mV) \n",
    "## Savatzky-Golay smoothig \n",
    "    * use the list comprehension for the sg-smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e29ff1d1-7c00-4e4a-abba-8231eb48fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def savgolFilter_xr(xrdata,window_length=7,polyorder=3): \n",
    "    # window_length = odd number\n",
    "    #import copy\n",
    "    #xrdata_prcssd = copy.deepcopy(xrdata)\n",
    "    xrdata_prcssd = xrdata.copy()\n",
    "    print('Apply a Savitzky-Golay filter to an xarray Dataset.')\n",
    "\n",
    "    for data_ch in xrdata:\n",
    "\n",
    "        if len(xrdata[data_ch].dims) == 2:\n",
    "            print('3D data')\n",
    "            # smoothing filter only for the 3D data set\n",
    "            # ==> updaded \n",
    "            xrdata_prcssd[data_ch]\n",
    "            ### 2D data case \n",
    "            ### assume that coords are 'X','Y','bias_mV'\n",
    "            #### two case X,bias_mV or Y,bias_mV \n",
    "            if 'X' in xrdata[data_ch].dims :\n",
    "                x_axis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "                # xrdata is X,bias_mV \n",
    "                # use the isel(X = x) \n",
    "                xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                    np.array (\n",
    "                        [sp.signal.savgol_filter(xrdata[data_ch].isel(X = x).values,\n",
    "                                                 window_length, \n",
    "                                                 polyorder , \n",
    "                                                 mode = 'nearest')\n",
    "                         for x in range(x_axis)]),\n",
    "                    dims = [\"X\", \"bias_mV\"],\n",
    "                    coords = {\"X\": xrdata.X,\n",
    "                              \"bias_mV\": xrdata.bias_mV})\n",
    "            elif 'Y' in xrdata[data_ch].dims  :                # xrdata is XY,bias_mV                 # use the isel(Y = y) \n",
    "                y_axis = xrdata.Y.size\n",
    "                xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                    np.array (\n",
    "                        [sp.signal.savgol_filter(xrdata[data_ch].isel(Y = y).values,\n",
    "                                                 window_length, \n",
    "                                                 polyorder , \n",
    "                                                 mode = 'nearest')\n",
    "                         for y in range(y_axis) ]),\n",
    "                    dims = [\"Y\", \"bias_mV\"],\n",
    "                    coords = {\"Y\": xrdata.Y,\n",
    "                              \"bias_mV\": xrdata.bias_mV}\n",
    "                )\n",
    "            else: pass\n",
    "            \n",
    "        elif len(xrdata[data_ch].dims) == 3:\n",
    "            x_axis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "            y_axis = xrdata.Y.size\n",
    "            print (data_ch)\n",
    "            xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                np.array ([\n",
    "                    sp.signal.savgol_filter(xrdata[data_ch].isel(X = x, Y = y).values,\n",
    "                                            window_length, \n",
    "                                            polyorder , \n",
    "                                            mode = 'nearest')\n",
    "                    for x in range(x_axis) \n",
    "                    for y in range(y_axis)\n",
    "                ] ).reshape(x_axis,y_axis, xrdata.bias_mV.size),\n",
    "                dims = [\"X\", \"Y\", \"bias_mV\"],\n",
    "                coords = {\"X\": xrdata.X,\n",
    "                          \"Y\": xrdata.Y,\n",
    "                          \"bias_mV\": xrdata.bias_mV}            )\n",
    "        else : pass\n",
    "    return xrdata_prcssd\n",
    "\n",
    "#grid_2D_sg = savgolFilter_xr(grid_2D)\n",
    "#grid_2D_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a4d821b-49c5-4e66-8f29-8ccab17abd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_xr(xrdata, distance = None, threshold = None): \n",
    "    from scipy.signal import find_peaks\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Find peaks in STS to an xarray Dataset.')\n",
    "\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "                    # ==> updaded             \n",
    "            \n",
    "            \n",
    "\n",
    "            ### 2D data case \n",
    "            ### assume that coords are 'X','Y','bias_mV'\n",
    "            #### two case X,bias_mV or Y,bias_mV \n",
    "            if 'X' in xrdata[data_ch].dims :\n",
    "                # xrdata is X,bias_mV \n",
    "                # use the isel(X = x) \n",
    "                x_axis = xrdata.X.size\n",
    "\n",
    "                #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                    np.array([ find_peaks(xrdata[data_ch].isel(X = x).values, \n",
    "                                          distance = distance,\n",
    "                                          threshold = threshold)\n",
    "                              for x in range(x_axis)], dtype = object )[:,0],\n",
    "                dims=[\"X\"],\n",
    "                coords={\"X\": xrdata.X})\n",
    "            \n",
    "            elif 'Y' in xrdata[data_ch].dims :\n",
    "                # xrdata is Y,bias_mV \n",
    "                # use the isel(Y = y) \n",
    "                y_axis = xrdata.Y.size\n",
    "\n",
    "                #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                    np.array([ find_peaks(xrdata[data_ch].isel(Y = y).values,\n",
    "                                          distance = distance,\n",
    "                                          threshold = threshold)\n",
    "                              for y in range(y_axis)], dtype = object )[:,0],\n",
    "                dims=[\"Y\"],\n",
    "                coords={\"Y\": xrdata.Y})\n",
    "            \n",
    "            # ==> updated \n",
    "            \n",
    "        elif len(xrdata[data_ch].dims) == 3:\n",
    "            \n",
    "            x_axis = xrdata.X.size\n",
    "            y_axis = xrdata.Y.size\n",
    "            print (data_ch)\n",
    "            \"\"\"xrdata_prcssd[data_ch+'_peaks']= xr.DataArray(np.ones((xAxis,yAxis), dtype = object),\n",
    "                                                             dims=[\"X\", \"Y\"],\n",
    "                                                             coords={\"X\": xrdata.X, \"Y\": xrdata.Y} )\"\"\"\n",
    "            xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                np.array([ find_peaks(xrdata[data_ch].isel(X = x, Y = y).values,\n",
    "                                      distance = distance,\n",
    "                                      threshold = threshold)[0] \n",
    "                          for x in range(x_axis)  \n",
    "                          for y in range(y_axis)], dtype = object ).reshape(x_axis,y_axis),\n",
    "                dims=[\"X\", \"Y\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y})         \n",
    "        else : pass\n",
    "    return xrdata_prcssd\n",
    "#grid_2D_sg_pks = find_peaks_xr(grid_2D_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644e233-ba1b-4c60-b8dc-52b647f61bea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def peak_pad(xrdata):\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    if len(xrdata.dims)==2:\n",
    "        for data_ch in xrdata:\n",
    "        \n",
    "               \n",
    "            # smoothing filter only for the 3D data set\n",
    "            ### 2D data case \n",
    "            ### assume that coords are 'X','Y','bias_mV'\n",
    "            #### two case X,bias_mV or Y,bias_mV \n",
    "            if 'X' in xrdata[data_ch].dims :\n",
    "                # xrdata is X,bias_mV \n",
    "                # use the isel(X = x) \n",
    "                x_axis = xrdata.X.size\n",
    "\n",
    "                if data_ch.endswith('_peaks'):\n",
    "                    peaks = xrdata[data_ch].values\n",
    "                    peaks_count_max = max([ len(peaks_r) \n",
    "                                    for peaks_r in peaks])\n",
    "                    padding_value = np.nan\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                    \n",
    "                    peaks_pad = np.array(\n",
    "                        [ np.pad(peaks_r.astype(float), \n",
    "                                 (0,peaks_count_max-len(peaks_r)), mode = 'constant', \n",
    "                               constant_values = padding_value)\n",
    "                        for peaks_r in peaks ]\n",
    "                    ).reshape((x_axis,-1))\n",
    "                    \n",
    "                    xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"peaks\": np.arange(peaks_count_max)})\n",
    "                else: pass###\n",
    " \n",
    "            elif 'Y' in xrdata[data_ch].dims :\n",
    "                # xrdata is Y,bias_mV \n",
    "                # use the isel(Y = y) \n",
    "                y_axis = xrdata.Y.size\n",
    "                if data_ch.endswith('_peaks'):\n",
    "                    peaks = xrdata[data_ch].values\n",
    "                    peaks_count_max = max([ len(peaks_r) \n",
    "                                    for peaks_r in peaks])\n",
    "                    padding_value = np.nan\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                    \n",
    "                    peaks_pad = np.array(\n",
    "                        [ np.pad(peaks_r.astype(float), \n",
    "                                 (0,peaks_count_max-len(peaks_r)), mode = 'constant', \n",
    "                               constant_values = padding_value)\n",
    "                        for peaks_r in peaks ]\n",
    "                    ).reshape((y_axis,-1))\n",
    "                    \n",
    "                    xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"Y\", \"peaks\"],\n",
    "                    coords={\"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)})\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                else: pass #\n",
    "            else: pass##\n",
    "\n",
    "       \n",
    "    \n",
    "        #if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            \n",
    "    else : \n",
    "        x_axis = xrdata.X.size\n",
    "        y_axis = xrdata.Y.size\n",
    "        for data_ch in xrdata_prcssd:\n",
    "            if data_ch.endswith('_peaks'):\n",
    "                peaks = xrdata[data_ch].values\n",
    "                peaks_count_max = max([ len(peaks_r_c) \n",
    "                                for peaks_r in peaks\n",
    "                                for  peaks_r_c in peaks_r])\n",
    "                padding_value = np.nan\n",
    "\n",
    "                peaks_pad = np.array([\n",
    "                    np.pad(peaks_r_c.astype(float), \n",
    "                           (0,peaks_count_max-len(peaks_r_c)),\n",
    "                           mode = 'constant', \n",
    "                           constant_values = padding_value)\n",
    "                    for peaks_r in peaks \n",
    "                    for  peaks_r_c in peaks_r]).reshape((x_axis,y_axis,-1))\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"Y\",\"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)})\n",
    "            else: pass\n",
    "    return xrdata_prcssd\n",
    "        \n",
    "#grid_3D_sg_pks_pad = peak_pad(grid_3D_sg_pks)\n",
    "#grid_3D_sg_pks_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0ed68-e77c-457f-8af3-cf1306425a16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def peak_pad(xrdata):\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    xAxis = xrdata.X.size\n",
    "    yAxis = xrdata.Y.size\n",
    "    for ch in xrdata_prcssd:\n",
    "        if ch.endswith('_peaks'):\n",
    "            peaks = xrdata[ch].values\n",
    "            peaks_count_max = max([ len(peaks_r_c) \n",
    "                            for peaks_r in peaks\n",
    "                            for  peaks_r_c in peaks_r])\n",
    "            padding_value = np.nan\n",
    "            \n",
    "            peaks_pad = np.array([\n",
    "                np.pad(peaks_r_c.astype(float), \n",
    "                       (0,peaks_count_max-len(peaks_r_c)),\n",
    "                       mode = 'constant', \n",
    "                       constant_values = padding_value)\n",
    "                for peaks_r in peaks \n",
    "                for  peaks_r_c in peaks_r]).reshape((xAxis,yAxis,-1))\n",
    "            \n",
    "            xrdata_prcssd[ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"Y\",\"peaks\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)})\n",
    "        else: pass\n",
    "    return xrdata_prcssd\n",
    "        \n",
    "#grid_3D_sg_pks_pad = peak_pad(grid_3D_sg_pks)\n",
    "#grid_3D_sg_pks_pad\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a346c44-dd11-47b1-aeb4-13a11b512294",
   "metadata": {},
   "source": [
    "### Rotating the 3D data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c87aa-40d6-4026-a11f-059ca7760913",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3D rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e099e-5a39-438e-8b53-938383c99a6a",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Xr rotation function \n",
    "# rotate the XY plan in xr data \n",
    "def rotate_3D_xr (xrdata, rotation_angle): \n",
    "    # padding first \n",
    "    for ch_i,ch_name in enumerate (xrdata):\n",
    "        if ch_i == 0:  # use only the first channel to calculate a padding size \n",
    "            padding_shape = skimage.transform.rotate(xrdata[ch_name].values.astype('float64'),\n",
    "                                                     rotation_angle,\n",
    "                                                     resize = True).shape[:2]\n",
    "            # After rotation, still 3D shape ->  [:2]\n",
    "            \n",
    "            padding_xy = (np.array( padding_shape)-np.array(xrdata[ch_name].shape[:2]) +1)/2\n",
    "            padding_xy = padding_xy.astype(int)\n",
    "    xrdata_pad = xrdata.pad(X=(padding_xy[0],padding_xy[0]), \n",
    "                            Y =(padding_xy[1],padding_xy[1]),\n",
    "                            mode='constant',\n",
    "                            cval = xrdata.min())\n",
    "    if np.array(xrdata_pad[ch_name]).shape[:2] != padding_shape:\n",
    "        # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.X).mean()\n",
    "        y_spacing = np.diff(xrdata.Y).mean()\n",
    "        xrdata.X[0]\n",
    "        xrdata.Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim+1)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim+1)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"X\" :  x_pad_arr}).assign_coords({\"Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.sel(X = xrdata_pad.X[:-1].values, Y = xrdata_pad.Y[:-1].values)\n",
    "        print ('padding size != rot_size')\n",
    "    else : # np.array(xrdata_pad[ch_name]).shape == padding_shape \n",
    "            # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.X).mean()\n",
    "        y_spacing = np.diff(xrdata.Y).mean()\n",
    "        xrdata.X[0]\n",
    "        xrdata.Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"X\" :  x_pad_arr}).assign_coords({\"Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.copy()      \n",
    "        print ('padding size == rot_size')\n",
    "    # call 1 channel\n",
    "        # use the list_comprehension for bias_mV range\n",
    "        # list comprehension is more faster\n",
    "        # after rotation, resize = False! , or replacement size error! \n",
    "        # replace the channel(padded) 3D data as a new 3D (rotated )data set \n",
    "\n",
    "    for ch in xrdata_pad:\n",
    "        xrdata_rot[ch].values = skimage.transform.rotate(xrdata[ch].values.astype('float64'),\n",
    "                                                         rotation_angle,\n",
    "                                                         cval =xrdata[ch].to_numpy().min(),\n",
    "                                                         resize = True)\n",
    "    return xrdata_rot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4d9f4-1e6e-4688-8ad1-bd792b501b68",
   "metadata": {},
   "source": [
    "### average X or Y direction jof Grid_3D dataset \n",
    "* use xr_data (3D)\n",
    "* average_in = 'X' or 'Y'\n",
    "* ch_l_name = channel name for line profile  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8823fae9-88c6-43fd-8176-28d3fe8096d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid3D_line_avg_pks (xr_data, average_in =  'X',\n",
    "                         ch_l_name = 'LIX_unit_calc',\n",
    "                         distance = None,\n",
    "                         threshold = None) : \n",
    "\n",
    "    if average_in ==  'X':\n",
    "        mean_direction = 'X'\n",
    "        line_direction = 'Y'\n",
    "        print('line_direction == Y')\n",
    "    elif average_in ==  'Y': \n",
    "        mean_direction = 'Y'\n",
    "        line_direction = 'X'\n",
    "        print('line_direction == X')\n",
    "    else: print ('check the line STS direction in 3D dataset ')\n",
    "\n",
    "    xr_data_l = xr_data.mean( dim = mean_direction )\n",
    "    xr_data_l.attrs = xr_data.attrs.copy()\n",
    "    # add attrs manually \n",
    "\n",
    "    ### find peaks & pad \n",
    "    #* use the SG filter \n",
    "    #* derivative (dim = 'bias_mV') twice \n",
    "    #* find peaks & padding \n",
    "\n",
    "    xr_data_l_pks=  peak_pad(\n",
    "        find_peaks_xr(\n",
    "            savgolFilter_xr(\n",
    "                savgolFilter_xr(\n",
    "                    xr_data_l.differentiate(coord='bias_mV')\n",
    "                ).differentiate(coord='bias_mV')\n",
    "            )*-1, distance = distance))\n",
    "    if average_in ==  'X':\n",
    "        xr_data_l_pks.attrs['line_direction'] ='Y'\n",
    "    elif average_in ==  'Y': \n",
    "        xr_data_l_pks.attrs['line_direction'] ='X'\n",
    "    else: print ('check the line STS direction in 3D dataset ')\n",
    "    # smooth, deriv, smooth, derive, find peak, padding \n",
    "    #xr_data_l_pks\n",
    "    \n",
    "    \n",
    "    # in the xr_data_l_pks\n",
    "    # choose a particular channel after pean & pad \n",
    "    # replace the channel to original xrdata \n",
    "    # xr_data_l_pks contains 2nd derivative results \n",
    "    \n",
    "    for ch_names in xr_data:\n",
    "        xr_data_l_pks[ch_names] =  xr_data_l [ch_names]\n",
    "    \n",
    "    \n",
    "    return xr_data_l_pks\n",
    "#grid_3D_test_l_pk = grid3D_line_avg_pks(grid_3D, average_in= 'Y')\n",
    "#grid_3D_test_l_pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a95aef-3339-448f-bcf2-3d67e552b796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "452c12dc-b3f3-422d-a0b3-75992042d9be",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  grid_lineNpks_offset(xr_data_l_pks, \n",
    "                          ch_l_name = 'LIX_unit_calc',\n",
    "                          plot_y_offset= 2E-11, \n",
    "                          peak_LIX_min = 1E-13, \n",
    "                          fig_size = (6,8), \n",
    "                          legend_title = None):\n",
    "    # add peak point one-by-one (no palett func in sns)\n",
    "    #  after find peak & padding\n",
    "    # use choose the channel to offset-plot \n",
    "    # use the plot_y_offset to adjust the offset values \n",
    "    ch_l_name = ch_l_name\n",
    "    ch_l_pk_name = ch_l_name +'_peaks_pad'\n",
    "    line_direction = xr_data_l_pks.line_direction\n",
    "    plot_y_offset  =  plot_y_offset\n",
    "    \n",
    "    sns_color_palette = \"rocket\"\n",
    "    #color map for fig\n",
    "    \n",
    "    #xr_data_l_pks\n",
    "    ### prepare XR dataframe for line spectroscopy plot \n",
    "    xr_data_l_pks_ch_slct = xr_data_l_pks[[ch_l_name,ch_l_pk_name]]\n",
    "    # choose the 2 channels from 2nd derivative (to maintain the coords info) \n",
    "\n",
    "\n",
    "    #line_direction check again \n",
    "    \n",
    "    if xr_data_l_pks.line_direction == 'Y': \n",
    "        spacing = xr_data_l_pks_ch_slct.Y_spacing\n",
    "    elif xr_data_l_pks.line_direction == 'X': \n",
    "        spacing = xr_data_l_pks_ch_slct.X_spacing\n",
    "    else : \n",
    "        print('check direction & X or Y spacing for offset') \n",
    "\n",
    "    xr_data_l_pks_ch_slct['offset'] = (xr_data_l_pks_ch_slct[line_direction] - xr_data_l_pks_ch_slct[line_direction].min())/spacing\n",
    "    # prepare offset index channnel \n",
    "    print (' plot_y_offset  to adjust line-by-line spacing')\n",
    "\n",
    "    xr_data_l_pks_ch_slct[ch_l_name+'_offset'] = xr_data_l_pks_ch_slct[ch_l_name] + plot_y_offset * xr_data_l_pks_ch_slct['offset']\n",
    "    # offset the curve b\n",
    "    print (xr_data_l_pks_ch_slct)\n",
    "    \n",
    "\n",
    "    ch_l_name_df_list = [] \n",
    "    ch_l_name_pks_df_list = []\n",
    "    # prepare empty list to append dataframes in the for - loop (y_i or x_i)\n",
    "\n",
    "    #line_direction check again \n",
    "    #########################\n",
    "    # line_diection check\n",
    "    if xr_data_l_pks_ch_slct.line_direction == 'Y': \n",
    "        lines  = xr_data_l_pks_ch_slct.Y\n",
    "\n",
    "        for y_i, y_points in enumerate (lines):\n",
    "\n",
    "            # set min peak height (LIX amplitude =  resolution limit)\n",
    "\n",
    "            y_i_pks  = xr_data_l_pks_ch_slct[ch_l_pk_name].isel(Y = y_i).dropna(dim='peaks').astype('int32')\n",
    "            # at (i_th )Y position, select peak index for bias_mV\n",
    "            real_pks_mask = (xr_data_l_pks_ch_slct.isel(Y = y_i, bias_mV = y_i_pks.values)[ch_l_name] > peak_LIX_min).values\n",
    "            # prepare a 'temp' mask for each Y position \n",
    "            y_i_pks_slct =  y_i_pks.where(real_pks_mask).dropna(dim='peaks').astype('int32')\n",
    "            # y_i_pks_slct with mask selection  \n",
    "\n",
    "            ch_l_name_y_i_df = xr_data_l_pks_ch_slct[ch_l_name+'_offset'].isel(Y = y_i).to_dataframe()\n",
    "            # LIX_offset  at Y_i position \n",
    "            ch_l_name_df_list.append(ch_l_name_y_i_df)\n",
    "            \n",
    "            ch_l_name_y_i_pks_df = xr_data_l_pks_ch_slct.isel(Y = y_i, bias_mV = y_i_pks_slct.values)[ch_l_name+'_offset'].to_dataframe()\n",
    "            # selected peaks with offest Y \n",
    "            ch_l_name_pks_df_list.append(ch_l_name_y_i_pks_df)\n",
    "            \n",
    "            # data at selected Y, & peak position, LIX_offset\n",
    "            \n",
    "    #########################\n",
    "    # line_diection check\n",
    "\n",
    "    elif xr_data_l_pks_ch_slct.line_direction == 'X': \n",
    "        lines = xr_data_l_pks_ch_slct.X\n",
    "\n",
    "        for x_i, x_points in enumerate (lines):\n",
    "\n",
    "            # set min peak height (LIX amplitude =  resolution limit)\n",
    "\n",
    "            x_i_pks  = xr_data_l_pks_ch_slct[ch_l_pk_name].isel(X = x_i).dropna(dim='peaks').astype('int32')\n",
    "            # at (i_th )X position, select peak index for bias_mV\n",
    "            real_pks_mask = (xr_data_l_pks_ch_slct.isel(X = x_i, bias_mV = x_i_pks.values)[ch_l_name] > peak_LIX_min).values\n",
    "            # prepare a 'temp' mask for each X position \n",
    "            x_i_pks_slct =  x_i_pks.where(real_pks_mask).dropna(dim='peaks').astype('int32')\n",
    "            # x_i_pks_slct with mask selection  \n",
    "\n",
    "            ch_l_name_x_i_df = xr_data_l_pks_ch_slct[ch_l_name+'_offset'].isel(X = x_i).to_dataframe()\n",
    "            # LIX_offset  at X_i position \n",
    "            ch_l_name_df_list.append(ch_l_name_x_i_df)\n",
    "            ch_l_name_x_i_pks_df = xr_data_l_pks_ch_slct.isel(X = x_i, bias_mV = x_i_pks_slct.values)[ch_l_name+'_offset'].to_dataframe()\n",
    "            ch_l_name_pks_df_list.append(ch_l_name_x_i_pks_df)\n",
    "            \n",
    "            # selected peaks with offest X \n",
    "            \n",
    "    else : \n",
    "        print('check direction & X or Y spacing for offset') \n",
    "    \n",
    "    ch_l_name_df = pd.concat(ch_l_name_df_list).reset_index()\n",
    "    ch_l_name_pks_df = pd.concat(ch_l_name_pks_df_list).reset_index()\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize = fig_size)\n",
    "\n",
    "    sns.lineplot(data = ch_l_name_df,\n",
    "                         x ='bias_mV', \n",
    "                         y = ch_l_name+'_offset',\n",
    "                         palette = \"rocket\",\n",
    "                         hue = xr_data_l_pks.line_direction,\n",
    "                         ax = ax)\n",
    "\n",
    "    sns.scatterplot(data = ch_l_name_pks_df,\n",
    "                            x ='bias_mV',\n",
    "                            y = ch_l_name+'_offset',\n",
    "                            palette =\"rocket\",\n",
    "                            hue = xr_data_l_pks.line_direction,\n",
    "                            ax = ax)\n",
    "    # legend control!( cut the handles 1/2)\n",
    "    ax.set_xlabel('Bias (mV)')   \n",
    "    #ax.set_ylabel(ch_l_name+'_offset')   \n",
    "    ax.set_ylabel('LDOS')   \n",
    "    handles0, labels0 = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles0[:int(len(handles0)//2)],\n",
    "              labels0[:int(len(labels0)//2)], title = legend_title)\n",
    "    # use the half of legends (line + scatter) --> use lines only\n",
    "    #plt.show()\n",
    "    return xr_data_l_pks_ch_slct, ch_l_name_df, ch_l_name_pks_df, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfae91-0512-491c-8a72-5afe85aae299",
   "metadata": {},
   "source": [
    "### Plot line profile (line offset + peak positions) \n",
    "### only after after **grid3D_line_avg_pks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6156c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe952a49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d08e017f-8f46-4834-86e6-3ebfde19eb67",
   "metadata": {},
   "source": [
    "### Not Used Previous (WORKING)Functions (FOR 3d ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee9607-306e-46c4-840d-94d47308aa79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def savgolFilter_xr(xrdata,window_length=7,polyorder=3): \n",
    "    # window_length = odd number\n",
    "    #import copy\n",
    "    #xrdata_prcssd = copy.deepcopy(xrdata)\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Apply a Savitzky-Golay filter to an xarray Dataset.')\n",
    "    xAxis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "    yAxis = xrdata.Y.size\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims) == 2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            pass\n",
    "            \n",
    "        else :\n",
    "            print (data_ch)\n",
    "            xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                np.array ([\n",
    "                    sp.signal.savgol_filter(xrdata[data_ch].isel(X = x, Y = y).values,\n",
    "                                            window_length, \n",
    "                                            polyorder , \n",
    "                                            mode = 'nearest')\n",
    "                    for x in range(xAxis) \n",
    "                    for y in range(yAxis)\n",
    "                ] ).reshape(xAxis,yAxis, xrdata.bias_mV.size),\n",
    "                dims = [\"X\", \"Y\", \"bias_mV\"],\n",
    "                coords = {\"X\": xrdata.X,\n",
    "                          \"Y\": xrdata.Y,\n",
    "                          \"bias_mV\": xrdata.bias_mV}\n",
    "            )\n",
    "    return xrdata_prcssd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c7e9d-ee10-4070-a670-44afaf44671a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def find_peaks_xr(xrdata): \n",
    "    from scipy.signal import find_peaks\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Find peaks in STS to an xarray Dataset.')\n",
    "    xAxis = xrdata.X.size\n",
    "    yAxis = xrdata.Y.size\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            pass\n",
    "            \n",
    "        else :\n",
    "            print (data_ch)\n",
    "            \"\"\"xrdata_prcssd[data_ch+'_peaks']= xr.DataArray(np.ones((xAxis,yAxis), dtype = object),\n",
    "                                                             dims=[\"X\", \"Y\"],\n",
    "                                                             coords={\"X\": xrdata.X, \"Y\": xrdata.Y} )\"\"\"\n",
    "            xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                np.array([ find_peaks(xrdata[data_ch].isel(X = x, Y = y).values)[0] \n",
    "                          for x in range(xAxis)  \n",
    "                          for y in range(yAxis)], dtype = object ).reshape(xAxis,yAxis),\n",
    "                dims=[\"X\", \"Y\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y})\n",
    "            \n",
    "                    # use the \"xrdata_prcssd[data_ch].values[x,y,:]\"  for \"xrdata_prcssd\" \n",
    "                    # not \".isel(X = x, Y = y).values\"\"\"\n",
    "\n",
    "    return xrdata_prcssd\n",
    "#grid_3D_sg_pks = find_peaks_xr(grid_3D_sg)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
